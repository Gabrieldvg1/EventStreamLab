bundle:
  name: eventstreamlab

# Single-environment setup using Databricks Bundles.
targets:
  default:
    mode: development
    default: true

    # Point this directly at your Databricks workspace.
    workspace:
      host: {}

    resources:
      # Delta Live Tables pipeline that reads from Event Hubs (Kafka endpoint)
      pipelines:
        events_pipeline:
          name: "eventstreamlab_events_pipeline"

          # Use built-in hive metastore (no Unity Catalog). Only set target (database).
          target: "default"

          libraries:
            - file:
                path: ./dlt/kafka_ingestion.py

          # Simple single-cluster definition for the pipeline.
          clusters:
            - label: default
              node_type_id: "Standard_D4s_v3"
              autoscale:
                min_workers: 1
                max_workers: 1

          configuration:
            # Kafka / Event Hubs configuration passed to the DLT script.
            BOOTSTRAP_SERVER: "eventstreamlab-ns.servicebus.windows.net:9093"
            EVENT_HUB_TOPIC: "demo-events"

            # Where the Event Hubs connection string is stored in Databricks secrets.
            EVENT_HUB_SECRET_SCOPE: "event-hubs"
            EVENT_HUB_SECRET_KEY: "connection-string"

      # A job that can trigger the DLT pipeline
      jobs:
        run_events_pipeline:
          name: "Run EventStreamLab DLT pipeline"
          tasks:
            - task_key: run-events-pipeline
              pipeline_task:
                pipeline_id: ${resources.pipelines.events_pipeline.id}


